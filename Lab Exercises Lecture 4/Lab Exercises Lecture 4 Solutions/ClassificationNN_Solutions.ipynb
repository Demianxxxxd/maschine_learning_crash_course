{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying movie reviews\n",
    "\n",
    "In this example, we will learn to classify movie reviews into \"positive\" reviews and \"negative\" reviews, just based on the text content of the reviews. This is a \"binary classification example\".\n",
    "\n",
    "## The Dataset\n",
    "\n",
    "The IMDB dataset: a set of 50,000 reviews from the Internet Movie Database. They are split into 25,000 reviews for training and 25,000 reviews for testing, each set consisting in 50% negative and 50% positive reviews.\n",
    "\n",
    "The IMDB dataset comes pre-built in Keras. It has already been preprocessed: the reviews (sequences of words) have been turned into sequences of integers, where each integer stands for a specific word in a dictionary.\n",
    "\n",
    "The following code will load the dataset (when you run it for the first time, about 80MB of data will be downloaded to your machine):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "import numpy as np\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Check training and test data dimension. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,)\n",
      "(25000,)\n",
      "(25000,)\n",
      "(25000,)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)\n",
    "print(train_labels.shape)\n",
    "print(test_data.shape)\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 0, 1, 0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 14,\n",
       " 22,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 973,\n",
       " 1622,\n",
       " 1385,\n",
       " 65,\n",
       " 458,\n",
       " 4468,\n",
       " 66,\n",
       " 3941,\n",
       " 4,\n",
       " 173,\n",
       " 36,\n",
       " 256,\n",
       " 5,\n",
       " 25,\n",
       " 100,\n",
       " 43,\n",
       " 838,\n",
       " 112,\n",
       " 50,\n",
       " 670,\n",
       " 2,\n",
       " 9,\n",
       " 35,\n",
       " 480,\n",
       " 284,\n",
       " 5,\n",
       " 150,\n",
       " 4,\n",
       " 172,\n",
       " 112,\n",
       " 167,\n",
       " 2,\n",
       " 336,\n",
       " 385,\n",
       " 39,\n",
       " 4,\n",
       " 172,\n",
       " 4536,\n",
       " 1111,\n",
       " 17,\n",
       " 546,\n",
       " 38,\n",
       " 13,\n",
       " 447,\n",
       " 4,\n",
       " 192,\n",
       " 50,\n",
       " 16,\n",
       " 6,\n",
       " 147,\n",
       " 2025,\n",
       " 19,\n",
       " 14,\n",
       " 22,\n",
       " 4,\n",
       " 1920,\n",
       " 4613,\n",
       " 469,\n",
       " 4,\n",
       " 22,\n",
       " 71,\n",
       " 87,\n",
       " 12,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 38,\n",
       " 76,\n",
       " 15,\n",
       " 13,\n",
       " 1247,\n",
       " 4,\n",
       " 22,\n",
       " 17,\n",
       " 515,\n",
       " 17,\n",
       " 12,\n",
       " 16,\n",
       " 626,\n",
       " 18,\n",
       " 2,\n",
       " 5,\n",
       " 62,\n",
       " 386,\n",
       " 12,\n",
       " 8,\n",
       " 316,\n",
       " 8,\n",
       " 106,\n",
       " 5,\n",
       " 4,\n",
       " 2223,\n",
       " 5244,\n",
       " 16,\n",
       " 480,\n",
       " 66,\n",
       " 3785,\n",
       " 33,\n",
       " 4,\n",
       " 130,\n",
       " 12,\n",
       " 16,\n",
       " 38,\n",
       " 619,\n",
       " 5,\n",
       " 25,\n",
       " 124,\n",
       " 51,\n",
       " 36,\n",
       " 135,\n",
       " 48,\n",
       " 25,\n",
       " 1415,\n",
       " 33,\n",
       " 6,\n",
       " 22,\n",
       " 12,\n",
       " 215,\n",
       " 28,\n",
       " 77,\n",
       " 52,\n",
       " 5,\n",
       " 14,\n",
       " 407,\n",
       " 16,\n",
       " 82,\n",
       " 2,\n",
       " 8,\n",
       " 4,\n",
       " 107,\n",
       " 117,\n",
       " 5952,\n",
       " 15,\n",
       " 256,\n",
       " 4,\n",
       " 2,\n",
       " 7,\n",
       " 3766,\n",
       " 5,\n",
       " 723,\n",
       " 36,\n",
       " 71,\n",
       " 43,\n",
       " 530,\n",
       " 476,\n",
       " 26,\n",
       " 400,\n",
       " 317,\n",
       " 46,\n",
       " 7,\n",
       " 4,\n",
       " 2,\n",
       " 1029,\n",
       " 13,\n",
       " 104,\n",
       " 88,\n",
       " 4,\n",
       " 381,\n",
       " 15,\n",
       " 297,\n",
       " 98,\n",
       " 32,\n",
       " 2071,\n",
       " 56,\n",
       " 26,\n",
       " 141,\n",
       " 6,\n",
       " 194,\n",
       " 7486,\n",
       " 18,\n",
       " 4,\n",
       " 226,\n",
       " 22,\n",
       " 21,\n",
       " 134,\n",
       " 476,\n",
       " 26,\n",
       " 480,\n",
       " 5,\n",
       " 144,\n",
       " 30,\n",
       " 5535,\n",
       " 18,\n",
       " 51,\n",
       " 36,\n",
       " 28,\n",
       " 224,\n",
       " 92,\n",
       " 25,\n",
       " 104,\n",
       " 4,\n",
       " 226,\n",
       " 65,\n",
       " 16,\n",
       " 38,\n",
       " 1334,\n",
       " 88,\n",
       " 12,\n",
       " 16,\n",
       " 283,\n",
       " 5,\n",
       " 16,\n",
       " 4472,\n",
       " 113,\n",
       " 103,\n",
       " 32,\n",
       " 15,\n",
       " 16,\n",
       " 5345,\n",
       " 19,\n",
       " 178,\n",
       " 32]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the Data\n",
    "\n",
    "We cannot feed lists of integers into a neural network. We have to turn lists into tensors.\n",
    "\n",
    "We could pad our lists so that they all have the same length, and turn them into an integer tensor of shape (samples, word_indices), then use as first layer in our network a layer capable of handling such integer tensors (the Embedding layer, which we will cover in detail later).\n",
    "We could one-hot-encode our lists to turn them into vectors of 0s and 1s. Concretely, this would mean for instance turning the sequence [3, 5] into a 10,000-dimensional vector that would be all-zeros except for indices 3 and 5, which would be ones. Then we could use as first layer in our network a Dense layer, capable of handling floating point vector data.\n",
    "We will go with the latter solution. Let's vectorize our data, which we will do manually for maximum clarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    # Create an all-zero matrix of shape (len(sequences), dimension)\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.  # set specific indices of results[i] to 1s\n",
    "    return results\n",
    "\n",
    "# Our vectorized training data\n",
    "x_train = vectorize_sequences(train_data)\n",
    "# Our vectorized test data\n",
    "x_test = vectorize_sequences(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check one sample.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 1., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One sample\n",
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,)\n"
     ]
    }
   ],
   "source": [
    "# Our vectorized labels\n",
    "y_train = np.asarray(train_labels).astype('float32')\n",
    "print(y_train.shape)\n",
    "y_test = np.asarray(test_labels).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data type before:int64\n",
      "data type after:float32\n"
     ]
    }
   ],
   "source": [
    "print('data type before:{}\\ndata type after:{}'.format(train_labels.dtype, y_train.dtype))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input data is simply vectors, and the labels are scalars (1 and 0): this is the easiest setup you will ever encounter. \n",
    "\n",
    "*Key questions*: how many layers do we need? How many hidden units for each layer? \n",
    "We will build a simple stack of fully-connected (Dense) layers. The network should have the following structure:\n",
    "- Dense layer with 16 hidden units and ReLU activation function;\n",
    "- Dense layer with 16 hidden units and ReLU activation function;\n",
    "- Dense layer output with sigmoid activation function;\n",
    "\n",
    "*Remember*: for the fully connected layer, it holds: output = relu(dot(W, input) + b), with W weight matrix and b bias.\n",
    "\n",
    "*Remember*: the sigmoid function will give you a score between 0 and 1, which tells you \"how likely the sample is to have \"1\", that means the review to be \"positive\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import models and layers from Keras.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create the network model as described above.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_6 (Dense)             (None, 16)                160016    \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 16)                272       \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 160,305\n",
      "Trainable params: 160,305\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss: Binary Cross Entropy\n",
    "For a binary classification like our task, we use a loss function called Binary Cross-Entropy (BCE).\n",
    "\n",
    "$$BCE(y,\\hat{y}) =- y \\cdot log(\\hat y ) - (1- y) \\cdot log(1-\\hat y) $$\n",
    "\n",
    "where $y\\in\\mathbb{R}$ is the ground truth and $\\hat y\\in\\mathbb{R}$ is the predicted probability of the review to be positive\n",
    "\n",
    "Since the BCE function is a non-convex function, there is no closed-form solution for the optimal weights vector. In order to find the optimal parameters for our model, we need to use numeric methods such as Gradient Descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer and Gradient Descent\n",
    "\n",
    "The loss is a method of measuring how well our model fits the given data. The idea of the training process is to adjust iteratively the weights of our model in order to minimize the loss function. \n",
    "\n",
    "And this is where the optimizer comes in. In each training step, the optimizer updates the weights of the model w.r.t. the output of the loss function, thereby linking the loss function and model parameters together. The goal is to obtain a model which is accurately predicting the class for a new sample.\n",
    "\n",
    "\n",
    "Any discussion about optimizers needs to begin with the most popular one, and it’s called Gradient Descent. This algorithm is used across all types of Machine Learning (and other math problems) to optimize. It’s fast, robust, and flexible. Here’s how it works:\n",
    "\n",
    "\n",
    "0. Initialize the weights with random values.\n",
    "1. Calculate loss with the current weights and the loss function.\n",
    "2. Calculate the gradient of the loss function w.r.t. the weights.\n",
    "3. Update weights with the corresponding gradient.\n",
    "4. Iteratively perform Step 1 to 3 until converges.\n",
    "\n",
    "The name of the optimizer already hints at the required concept: We use gradients which are very useful for minimizing a function. The gradient of the loss function w.r.t to the weights $w$ of our model tells us how to change our weights $w$ in order to minimize our loss function. \n",
    "\n",
    "The weights are updated each step as follows:\n",
    "$$ w^{(n+1)} = w^{(n)} - \\alpha \\cdot \\frac {dL}{dw}, $$\n",
    "where $ \\frac {dL}{dw}$ is the gradient of your loss function w.r.t. the weights $w$ and $\\alpha$ is the learning rate which is a predefined positive scalar determining the size of the step.\n",
    "\n",
    "There are also other popular optimizers that you can choose from, like RMSProp and Adam, which are in general good choices for neural networks. Here in the following, we are going to use RMSProp as our optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Configure the model with an  optimizer and  a loss function. Check Keras documentation and try to understand which one suits better for your problem.** (`rmsprop`,`binary_crossentropy`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Do the same as in the previous cell but pass an optimizer class instance as the optimizer argument. Check Keras [documentation](https://keras.io/api/optimizers/sgd/) for more insights.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import optimizers\n",
    "\n",
    "model.compile(optimizer=optimizers.RMSprop(learning_rate=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Do the same as in the previous cell, but pass function objects as the loss or metrics arguments.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import metrics\n",
    "\n",
    "model.compile(optimizer=optimizers.RMSprop(learning_rate=0.001),\n",
    "              loss=losses.binary_crossentropy,\n",
    "              metrics=[metrics.binary_accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validating the approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create the Validation set of 10,000 samples out of the training data set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = x_train[:10000]\n",
    "partial_x_train = x_train[10000:]\n",
    "\n",
    "y_val = y_train[:10000]\n",
    "partial_y_train = y_train[10000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train the model for 20 epochs in mini-batches of 512 samples. Use the Validation set you have just created. Check Keras [documentation](https://keras.io/api/models/model_training_apis/) for more info. You could split the data yourself and use *validation_data*, or simply use the *validation_split* argument**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x158f45950> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x158f45950> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.5210 - binary_accuracy: 0.7772WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x14f093440> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x14f093440> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "30/30 [==============================] - 4s 102ms/step - loss: 0.5157 - binary_accuracy: 0.7809 - val_loss: 0.4087 - val_binary_accuracy: 0.8497\n",
      "Epoch 2/20\n",
      "30/30 [==============================] - 1s 31ms/step - loss: 0.3073 - binary_accuracy: 0.9014 - val_loss: 0.3037 - val_binary_accuracy: 0.8857\n",
      "Epoch 3/20\n",
      "30/30 [==============================] - 1s 26ms/step - loss: 0.2218 - binary_accuracy: 0.9260 - val_loss: 0.3005 - val_binary_accuracy: 0.8793\n",
      "Epoch 4/20\n",
      "30/30 [==============================] - 1s 26ms/step - loss: 0.1702 - binary_accuracy: 0.9451 - val_loss: 0.2749 - val_binary_accuracy: 0.8910\n",
      "Epoch 5/20\n",
      "30/30 [==============================] - 1s 46ms/step - loss: 0.1405 - binary_accuracy: 0.9541 - val_loss: 0.3214 - val_binary_accuracy: 0.8739\n",
      "Epoch 6/20\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.1151 - binary_accuracy: 0.9650 - val_loss: 0.3014 - val_binary_accuracy: 0.8825\n",
      "Epoch 7/20\n",
      "30/30 [==============================] - 1s 31ms/step - loss: 0.0961 - binary_accuracy: 0.9707 - val_loss: 0.3172 - val_binary_accuracy: 0.8838\n",
      "Epoch 8/20\n",
      "30/30 [==============================] - 1s 31ms/step - loss: 0.0761 - binary_accuracy: 0.9793 - val_loss: 0.3385 - val_binary_accuracy: 0.8811\n",
      "Epoch 9/20\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.0620 - binary_accuracy: 0.9853 - val_loss: 0.3596 - val_binary_accuracy: 0.8809\n",
      "Epoch 10/20\n",
      "30/30 [==============================] - 1s 30ms/step - loss: 0.0515 - binary_accuracy: 0.9861 - val_loss: 0.3898 - val_binary_accuracy: 0.8801\n",
      "Epoch 11/20\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 0.0434 - binary_accuracy: 0.9899 - val_loss: 0.4326 - val_binary_accuracy: 0.8756\n",
      "Epoch 12/20\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 0.0330 - binary_accuracy: 0.9925 - val_loss: 0.4840 - val_binary_accuracy: 0.8696\n",
      "Epoch 13/20\n",
      "30/30 [==============================] - 1s 31ms/step - loss: 0.0265 - binary_accuracy: 0.9953 - val_loss: 0.4807 - val_binary_accuracy: 0.8728\n",
      "Epoch 14/20\n",
      "30/30 [==============================] - 1s 29ms/step - loss: 0.0220 - binary_accuracy: 0.9956 - val_loss: 0.5116 - val_binary_accuracy: 0.8718\n",
      "Epoch 15/20\n",
      "30/30 [==============================] - 1s 26ms/step - loss: 0.0159 - binary_accuracy: 0.9984 - val_loss: 0.5588 - val_binary_accuracy: 0.8648\n",
      "Epoch 16/20\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 0.0140 - binary_accuracy: 0.9979 - val_loss: 0.5926 - val_binary_accuracy: 0.8656\n",
      "Epoch 17/20\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 0.0134 - binary_accuracy: 0.9974 - val_loss: 0.6295 - val_binary_accuracy: 0.8629\n",
      "Epoch 18/20\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 0.0059 - binary_accuracy: 0.9997 - val_loss: 0.6321 - val_binary_accuracy: 0.8683\n",
      "Epoch 19/20\n",
      "30/30 [==============================] - 1s 31ms/step - loss: 0.0076 - binary_accuracy: 0.9991 - val_loss: 0.6661 - val_binary_accuracy: 0.8675\n",
      "Epoch 20/20\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 0.0035 - binary_accuracy: 0.9999 - val_loss: 0.7094 - val_binary_accuracy: 0.8637\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Check the `.history` of the result of your fit and print it to see in which format it is and how it is possible to access to the values. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'binary_accuracy', 'val_loss', 'val_binary_accuracy'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Complete the #TO DO to obtain the plot of the training and validation accuracy. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['binary_accuracy'] #TO DO: take from the history dictionary of the model the training accuracy\n",
    "val_acc = history.history['val_binary_accuracy'] #TO DO: take from the history dictionary of the model the validation accuracy\n",
    "loss = history.history['loss'] #TO DO: take from the history dictionary of the model the training loss\n",
    "val_loss = history.history['val_loss'] #TO DO: take from the history dictionary of the model the validation loss \n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# Plot the Loss\n",
    "\n",
    "# \"bo\" is for \"blue dot\"\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss') #TO DO: use the training loss\n",
    "# b is for \"solid blue line\"\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss') #TO DO: use validation loss\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAvEUlEQVR4nO3deZhU1bX38e+iQZBBUUBFURpH1CgIHTSIisYBxeAQjSIxOAVBjTG5Ro1zotxr1ETjxeHFKYrkAl6Hi4kaFRNJ4gANMoiKIjbYooggk8yw3j/2KSiKU93VXVMPv8/z1FNVZ1x1uvqs2nufs7e5OyIiIqmaFDsAERGpm5QgREQklhKEiIjEUoIQEZFYShAiIhJLCUJERGIpQUjGzOwlMxuc62WLycwqzOz4PGzXzWzf6PVDZnZTJsvWYj+DzOyV2sYpUhXTfRANm5mtTHrbElgLbIzeX+ruowsfVd1hZhXAJe7+Wo6368B+7j4nV8uaWSnwKdDM3TfkJFCRKjQtdgCSX+7eOvG6qpOhmTXVSUfqCn0f6wZVMTVSZtbXzCrN7Foz+xJ43Mx2MrO/mNkiM/smet0paZ1/mNkl0esLzOxfZnZ3tOynZnZyLZftYmYTzWyFmb1mZveb2VNp4s4kxtvM7N/R9l4xs/ZJ8883s3lmttjMbqji+BxhZl+aWUnStDPMbEb0upeZvWVmS83sCzMbYWbbpdnWn8zs9qT3v4rWWWBmF6Us29/M3jWz5Wb2mZndmjR7YvS81MxWmtn3Esc2af3eZjbZzJZFz70zPTY1PM47m9nj0Wf4xsyeT5p3mplNiz7DJ2bWL5q+VXWemd2a+DubWWlU1Xaxmc0HXo+mPx39HZZF35GDk9bf3sx+H/09l0Xfse3N7K9m9rOUzzPDzE6P+6ySnhJE47YbsDPQGRhC+D48Hr3fC1gNjKhi/cOB2UB74E7gUTOzWiz7Z2AS0A64FTi/in1mEuN5wIXALsB2wNUAZnYQ8GC0/d2j/XUihru/DXwLHJey3T9HrzcCv4g+z/eA7wOXVRE3UQz9onhOAPYDUts/vgV+ArQF+gPDkk5sR0fPbd29tbu/lbLtnYG/AvdFn+0PwF/NrF3KZ9jm2MSo7jiPIlRZHhxt654ohl7Ak8Cvos9wNFCRZh9xjgEOBE6K3r9EOE67AFOB5CrRu4GeQG/C9/gaYBPwBPDjxEJm1g3YA3ixBnEIgLvr0UgehH/U46PXfYF1QIsqlu8OfJP0/h+EKiqAC4A5SfNaAg7sVpNlCSefDUDLpPlPAU9l+JniYrwx6f1lwMvR65uBMUnzWkXH4Pg0274deCx63YZw8u6cZtmrgOeS3juwb/T6T8Dt0evHgDuSlts/edmY7d4L3BO9Lo2WbZo0/wLgX9Hr84FJKeu/BVxQ3bGpyXEGOhJOxDvFLPf/EvFW9f2L3t+a+Dsnfba9q4ihbbTMjoQEthroFrNcc2AJoV0HQiJ5IB//Uw39oRJE47bI3dck3phZSzP7f1GRfTmhSqNtcjVLii8TL9x9VfSydQ2X3R1YkjQN4LN0AWcY45dJr1clxbR78rbd/Vtgcbp9EUoLZ5pZc+BMYKq7z4vi2D+qdvkyiuM/CaWJ6mwVAzAv5fMdbmZ/j6p2lgFDM9xuYtvzUqbNI/x6Tkh3bLZSzXHek/A3+yZm1T2BTzKMN87mY2NmJWZ2R1RNtZwtJZH20aNF3L7cfS0wDvixmTUBBhJKPFJDShCNW+olbP8BHAAc7u47sKVKI121US58AexsZi2Tpu1ZxfLZxPhF8rajfbZLt7C7v084wZ7M1tVLEKqqPiT8St0BuL42MRBKUMn+DIwH9nT3HYGHkrZb3SWHCwhVQsn2Aj7PIK5UVR3nzwh/s7Yx630G7JNmm98SSo8Ju8Usk/wZzwNOI1TD7UgoZSRi+BpYU8W+ngAGEar+VnlKdZxkRglCkrUhFNuXRvXZt+R7h9Ev8nLgVjPbzsy+B/wgTzH+L3CqmfWJGpR/S/X/A38GriScIJ9OiWM5sNLMugLDMoxhHHCBmR0UJajU+NsQfp2vierzz0uat4hQtbN3mm2/COxvZueZWVMzOwc4CPhLhrGlxhF7nN39C0LbwANRY3YzM0skkEeBC83s+2bWxMz2iI4PwDTg3Gj5MuCsDGJYSyjltSSU0hIxbCJU1/3BzHaPShvfi0p7RAlhE/B7VHqoNSUISXYvsD3h19nbwMsF2u8gQkPvYkK9/1jCiSHOvdQyRnefBVxOOOl/AXwDVFaz2v8Q2mted/evk6ZfTTh5rwAejmLOJIaXos/wOjAnek52GfBbM1tBaDMZl7TuKmA48G8LV08dkbLtxcCphF//iwmNtqemxJ2pe6n6OJ8PrCeUor4itMHg7pMIjeD3AMuAN9hSqrmJ8Iv/G+A3bF0ii/MkoQT3OfB+FEeyq4GZwGRCm8Pv2Pqc9iRwCKFNS2pBN8pJnWNmY4EP3T3vJRhpuMzsJ8AQd+9T7FjqK5UgpOjM7Ltmtk9UJdGPUO/8fJHDknosqr67DBhZ7FjqMyUIqQt2I1yCuZJwDf8wd3+3qBFJvWVmJxHaaxZSfTWWVEFVTCIiEkslCBERidWgOutr3769l5aWFjsMEZF6Y8qUKV+7e4e4eQ0qQZSWllJeXl7sMERE6g0zS737fjNVMYmISCwlCBERiaUEISIisZQgREQklhKEiIjEyluCMLPHzOwrM3svzXwzs/vMbE40HGCPpHn9zGx2NO+6fMUoIpKN0aOhtBSaNAnPo0dXt0b92n8+SxB/AvpVMf9kwlCC+xGGu3wQwiAhwP3R/IOAgdFQkSIidcbo0TBkCMybB+7heciQmp2ksznB52L/1clbgnD3iYQueNM5DXjSg7cJo1V1BHoRhqec6+7rgDHRsiLSwGT7C7iY699wA6xatfW0VavC9Ez3nc0JPtv9ZySf45kSRoB6L828vwB9kt5PABKDiDySNP18YEQV+xhCGHCmfK+99nIRqR+eesq9ZUv3cHoMj5Ytw/T6sL7Z1usmHmaZrd+5c/z6nTsXZv8JQLnXwTGp44Zn9Cqmx3L3ke5e5u5lHTrE3i0uImnU51/gxV5/r9TBYquZnmr+/JpNz/X+M1HMBFHJ1mPzdiKMqZtuuojkULZVHNmun+0JstjrDx8OLVtuPa1lyzA9E9me4LPdf0bSFS1y8aDqKqb+hHFtDTgCmBRNbwrMBboA2wHTgYMz2V/Pnj1rVrYSacSyreJo7Ou7h+qozp1DtU7nzplXTyXWzaaKK9v9J1BFFVM+k8P/EMb9XU8oFVwMDAWGRvONcLXSJ4RxZcuS1j0F+Ciad0Om+1SCkMYmmxNEtnXY2a5f7DaEXJygs5WLE3y2ipIgivFQgpDGJNsTXH3/BV4X1m8IlCBE6qhsTlDZnqAbwi9wyV5VCUJdbYgUSbEbeQcNgpEjoXNnMAvPI0eG6YVYX+q+BjUmdVlZmWvAIKkvSktDUkjVuTNUVOR/fREAM5vi7mVx81SCECmSYl9mKVIdJQhp1Ip5o1i218GrikfyrUGNSS1SE4k2gMTdtIk2AMjsJJvt+sOHb70+1LwEMGiQEoLkj9ogpNGqC20Ao0eHrh3mzw8lh+HDdcKXwqqqDUIJQhqtJk3C1UOpzGDTpvyvL1IXqJFaJEa2bQCF6CxNpJiUIKTRyvYqIF1FJA2dEoTUa9lcRaQbxUSqpjYIqbdSryKC8AteJ2mRzKkNQhqkggy5KNKIKUFIvZXtncgiUjUlCKm3dBWRSH4pQUi9pauIRPJLCULqLV1FJJJf6otJ6jX1RSSSPypBSFFl25uqiOSPShBSNNn2hioi+aUShBSN7mMQqduUIKRodB+DSN2mBCFFo/sYROo2JQjJSjaNzLqPQaRuU4KQWks0Ms+bFwbOSTQyZ5okdB+DSN2m3lyl1nIx5KaIFJd6c5W8UCOzSMOmBCG1pkZmkYZNCUJqTY3MIg2bEoTUmhqZRRo2dbUhWVFneSINl0oQIiISSwlCRERiKUE0cupuW0TSURtEI6butkWkKipBNGLqbltEqpLXBGFm/cxstpnNMbPrYubvZGbPmdkMM5tkZt9JmldhZjPNbJqZqf+MPNCd0CJSlbwlCDMrAe4HTgYOAgaa2UEpi10PTHP3Q4GfAH9MmX+su3dP10+IZEd3QotIVfJZgugFzHH3ue6+DhgDnJayzEHABAB3/xAoNbNd8xiTJNGd0CJSlXwmiD2Az5LeV0bTkk0HzgQws15AZ6BTNM+BV8xsipkNSbcTMxtiZuVmVr5o0aKcBd8Y6E5oEalKPq9isphpqX2L3wH80cymATOBd4EN0bwj3X2Bme0CvGpmH7r7xG026D4SGAmhu+9cBd9Y6E5oEUknnyWISmDPpPedgAXJC7j7cne/0N27E9ogOgCfRvMWRM9fAc8Rqqwkhe5jEJF8yWeCmAzsZ2ZdzGw74FxgfPICZtY2mgdwCTDR3ZebWSszaxMt0wo4EXgvj7HWS9mO6CYiUpW8JQh33wBcAfwN+AAY5+6zzGyomQ2NFjsQmGVmHxKudvp5NH1X4F9mNh2YBPzV3V/OV6z1le5jEJF80pCj9ViTJqHkkMoMNm0qfDwiUv9oyNEGSvcxiEg+KUHUY7qPQUTySQmiHtN9DCKST+rNtZ7TfQwiki8qQYiISCwlCBERiaUEISIisZQgREQklhKEiIjEUoIQEZFYShAiIhJLCUJERGIpQYiISCwliCLTgD8iUlepq40iSgz4kxjTITHgD6j7DBEpPpUgikgD/ohIXaYEUUTz59dsuohIISlBFJEG/BGRukwJoog04I+I1GVKEEWkAX9EpC7TVUxFpgF/RKSuUglCRERiKUGIiEgsJQgREYmlBCEiIrGUIEREJJYShIiIxFKCEBGRWNUmCDM71cyUSEREGplMTvznAh+b2Z1mdmC+AxIRkbqh2gTh7j8GDgM+AR43s7fMbIiZtcl7dCIiUjQZVR25+3LgGWAM0BE4A5hqZj/LY2wiIlJEmbRB/MDMngNeB5oBvdz9ZKAbcHWe4xMRkSLJpLO+s4F73H1i8kR3X2VmF+UnLBERKbZMEsQtwBeJN2a2PbCru1e4+4S8RSYiIkWVSRvE08CmpPcbo2kCjB4NpaXQpEl4Hj262BGJiORGJgmiqbuvS7yJXm+XycbNrJ+ZzTazOWZ2Xcz8nczsOTObYWaTzOw7ma5bF4weDUOGwLx54B6ehwxRkhCRhiGTBLHIzAYk3pjZacDX1a1kZiXA/cDJwEHAQDM7KGWx64Fp7n4o8BPgjzVYt+huuAFWrdp62qpVYbqISH2XSYIYClxvZvPN7DPgWuDSDNbrBcxx97lRqWMMcFrKMgcBEwDc/UOg1Mx2zXDdops/v2bTRUTqk0xulPvE3Y8gnMwPcvfe7j4ng23vAXyW9L4ympZsOnAmgJn1AjoDnTJcl2i9IWZWbmblixYtyiCs3Nlrr5pNFxGpTzK6Uc7M+gOXAb8ws5vN7OZMVouZ5inv7wB2MrNpwM+Ad4ENGa4bJrqPdPcydy/r0KFDBmHlzvDh0LLl1tNatgzTRUTqu2ovczWzh4CWwLHAI8BZwKQMtl0J7Jn0vhOwIHmB6A7tC6P9GPBp9GhZ3bp1waBB4fmGG0K10l57heSQmC4iUp9lUoLo7e4/Ab5x998A32Prk3c6k4H9zKyLmW1H6PRvfPICZtY2mgdwCTAxShrVrltXDBoEFRWwaVN4VnIQkYYikxvl1kTPq8xsd2Ax0KW6ldx9g5ldAfwNKAEec/dZZjY0mv8QcCDwpJltBN4HLq5q3Zp9NBERyUYmCeIFM2sL3AVMJbQFPJzJxt39ReDFlGkPJb1+C9gv03VFRKRwqkwQ0UBBE9x9KfCMmf0FaOHuywoRnIiIFE+VbRDuvgn4fdL7tUoOdc/8+bByZbGjEJGGJpNG6lfM7IfRVUZSh6xbB7fcAvvsA4cfDl9+WeyIRKQhySRB/JLQOd9aM1tuZivMbHme45JqTJkCZWXw29/CqaeGfqCOPho++6z6dUVEMpHJndRt3L2Ju2/n7jtE73coRHCyrTVr4Ne/DiWGxYvhhRfguefglVdg4cKQJD79tNhRikhDkMmNckfHTU8dQEjy7+234aKL4IMPwvPvfw9t24Z5vXvDhAlw4olw1FHw+uuw//5FDVdE6rlMLnP9VdLrFoSO9KYAx+Ulonro5Zdhu+3gmGOgpCT321+1Cm66Ce65Bzp1Cvs76aRtlysrg3/8A44/PpQkXnsNvvOdbZcTEclEJlVMP0h6nAB8B1iY/9Dqh48/hv794fvfh86d4ZprYObM3G3/n/+Ebt3gD38IY0289158ckg49FB4440wgFHfvvDuu7mLRUQal0xKEKkqCUlCgP/8z1B6uP/+0BZwzz1w113hRP3jH8N558Eesf3QVm3lytDWMGIEdOkSqo+Oy7DMduCBMHFiSFrHHhtKHEccUfMYsuUOy5eHtpLEY8mSrd8npu2wAxxwwNaPHdTSJVJU5h7bSeqWBcz+my09qTYBugMV7v7j/IZWc2VlZV5eXl6w/c2dG+r5f/azkBgAFi2CceNg1Ch45x0wCyf2H/8Yzjwzs5Pe66/DJZeExuaf/Swkodatax7fvHkhSSxcCH/9a6h2yrXly+GRR0LJJu7Ev3Fj+nXbtoWddw6PpUvD8dyUNLjtbruFRLH//lsnji5doGltftqIyDbMbIq7l8XOyyBBDE56u4GQHP6dw/hyptAJ4qc/DYlg7lzYffdt53/8cRh+9Kmn4JNPYPvt4bTTQrI48URo1mzr5Zcvh1/9CkaOhH33hcceCw3O2ViwICSJefNg/PjQPpELS5bAfffBH/8YTu577AHt24eTfbt2Wx7p3u+007Yn+XXrwnGaPXvbx+LFW5Zr1izc+5GcNDp0CNVqJSXpH1XNb948JOFWrZR8pHHJNkG0Ata4+8bofQnQ3N1XVbliERQyQcybF07iw4aFE2VV3MMVSE89BWPHhpNdhw5w7rkhWXz3u+Ey1Z/+FCor4Ze/DPc3pI41UVtffQUnnBBOtP/7v+G+iWy29Yc/hCq1lSvh9NNDd+dlsV+v3Fm8OD5xzJkD69fndl/JySKT59atYe+9QzJXtZjUN9kmiLeB4919ZfS+NfCKu/fOeaRZKmSCGDoUHn88/OLt1Cnz9datC20CTz0VftGvXRvWr6yErl3DNvPRXrBkSWjcnjYNxoyBH/6wZut//nloWxk5MtyLcc45cP31cMghuY+1JjZsCN2sL10aqqc2bkz/qGr+2rUh4X37bXhOfp3ueeXKravESkqgZ8/Q7tO3L/TpU7uqQZFCyjZBTHP37tVNqwsKlSA++yxUcVxyCTzwQO23s2wZPPMMPPssHHZY+CXeokXu4ozb3ymnhLaRJ57IbOyKTz+F3/0uJK6NG+H88+G660K1TmPnHhLLihXhyrW//z08Jk0KpZqmTUPp8Nhjw6N379yVCkVypaoEgbtX+QD+DfRIet8TeKu69Yrx6NmzpxfC5Ze7N2vmPm9eQXaXUytWuPft627m/sgj6ZebPdt98GD3khL37bZzHzrUfe7cgoVZr61c6f7KK+6//rX7EUeEYwjhO3PUUe433+z++uvuq1cXO1IRd6Dc05xTMylBfBcYw5YhPzsC57j7lJykrxwqRAni889DffPgwaG6pT5avTpcUfXyy+Ey2ssv3zJv5swwbOq4caE0c+mlcPXVtbtUV4IVK+Bf/9pSwpg6NVRNNW8O3/teKF306xdKG+oSUwotqyqmaAPNgAMAAz509xw3C+ZGIRLEVVeFBtqPP4bS0rzuKq/Wrg3tCP/3f6FtoW9fuP328L51a7jiCvjFL2CXXYodacOzdGm4ATKRMKZPD9VVpaXwox+FR48eShZSGNm2QVwOjPYwaBBmthMw0N2zqH3Pj3wniC++CKWH886DRx/N224KZv36cBXVuHHhfdu28POfw5VXhstRpTASnS6OGwevvhoa3vfZJySKc84JN10qWUi+5KOR+l13Pyx3IeZGvhPEf/xHuO5/9uzwD9wQbNwIt94aSg3DhukyzWJbvDjckT9uXLhhcuPGcEFAIlkcfHD2+1izJnT4OGPGlseyZeFvv8MOsOOO8c9x09q0CfeXSP2VbYKYAXSLGjMS90HMcPccfFVzK58J4quvQhXA2WeHK4BE8m3RonCF29ixoX+tTZvgoINCovjRj8Jl0VVxD21m06dvnQxmz95yh3uLFqFDx3btQlvJsmXhhs1ly8L7DGqgadMmxHXxxeHenjZtsv/sUjjZJoi7gFLgIUKXG0OB+e5+dY7jzFo+E8S118Ldd4dfXupGWwrtyy/DJdFjx4YGb/dQ9ZQoWXTsCLNmbZ0IZsyAb77Zso3S0rBO8mPffdP3QLxpU7jnIzlpJD8nXi9dGvoKmzUr3Dw4cGC46VON7vVDtgmiCTAEOJ7QSP0u0NHdL69yxSLIV4L4+uvwz3X66eEGN5Fi+vzzcEf8uHHw5pthmtmWX/utWm2bCA45JFQL5Uuit4CHHw5JbNWqsN+f/jTcb7PTTvnbt2QnF1cxdQfOA84B5gLPuPuIXAaZC/lKENdfD3fcEX4hHXhgzjcvUmvz54eSxfLlW5JBly7FbRdYvhz+/OeQLKZODdVYZ58dkkWfPg2nVLF6dSg5jR8f+mNr1Srz7llSn9u0yc9YMpmoVYIws/2Bc4GBwGJgLHC1u3fOV6DZykeCWLIkjPPQv3/ookJEMjd1akgUo0eHNo2uXUMPBD/5SeiPrL5J9Iw8fnzoP2316nByP/jg8Dq1O5ZM2nAg3GHfs2cYSjjx6NSpMMm0tgliE/BP4GJ3nxNNm+vue+ct0izlI0HcfDPcdlu4gUyjs4nUzrffhiqxhx+Gt94KPfKecUYoVRx3XGYlnvXr0/eN1bx5aBvM9UnVHd5/P1yGPH58qEZzhz33hAEDwuOYY8L+49Zdsyazfr3mzQtd4Lz7brhHCUJ398kJo6wsP1cZ1jZBnEEoQfQGXibcTf2Iu3fJfYi5kesEsXRpKD2ceCI8/XTONivSqM2aFRLFqFGhhL733mGsklWr4jtETLzOpNfeli23HT8kMaZIpldXrV8fLgQYP35L9RGEE/SAAfCDH4RRHvPx637dunDV2aRJIWG88w589FGYZxaquJOTxne+k3339Lno7vt0QlXTccATwHPu/kp2YeVerhPEb34T7hGYNi18IUQkd9asCfd8PPJIOAkmd59ek67WW7UKCSS1K/iKiq2reHbffdvEccAB4UfgypWh65nx4+HFF8OPw+bNw1gqAwaELvKL1d3MN9+EhJGcNL7+OszbfvtQNXXEEaFTzdq0PWXdSJ20oZ2Bswl9MWU4AGbh5DJBLFsWrlzq2zd8iUWkflmzJowXkkgYH3205XXy5b/Nm4f7QjZsCO0i/fuHpHDCCXWzu3b30MtyIllMmhRKWdOn1257OUsQdV0uE8Tw4XDjjTBlSugXR0QaBvfwCzy5tNG0aSglHH548a4myoZ77au8qkoQGlwxxooVYdS0U09VchBpaMxCSaFDh3DZbUOQr6ud1ItKjAceCI1nN99c7EhERIpHCSLFypWhS42TTw5dBYiINFZKECkeeijUT950U7EjEREpLiWIJKtWhcFzTjghjPQlItKYKUEkGTkydOt9yy3FjkREpPiUICKrV4cbTY47Do48stjRiIgUX14ThJn1M7PZZjbHzK6Lmb+jmb1gZtPNbJaZXZg0r8LMZprZNDPL70DThLs5v/xSVy6JiCTk7T6IaOS5+4ETgEpgspmNd/f3kxa7HHjf3X9gZh2A2WY22t3XRfOPdfev8xVjwpo1oTvvo48OHW+JiEh+b5TrBcxx97kAZjYGOA1IThAOtDEzA1oDS4ANeYwp1uOPw4IF8OSThd6ziEjdlc8qpj2Az5LeV0bTko0ADgQWADOBn7v7pmieA6+Y2RQzG5KvINetg//6r9DucFyd611KRKR48lmCiLv5O7Xjp5OAaYReYvcBXjWzf7r7cuBId19gZrtE0z9094nb7CQkjyEAe+21V42D3LgRhg4Nl7U2lJGuRERyIZ8liEpgz6T3nQglhWQXAs96MAf4FOgK4O4LouevgOcIVVbbcPeR7l7m7mUdajFE1fbbhyFFjz22xquKiDRo+UwQk4H9zKyLmW1HGHxofMoy84HvA5jZrsABwFwza2VmbaLprYATgffyGKuIiKTIWxWTu28wsyuAvwElwGPuPsvMhkbzHwJuA/5kZjMJVVLXuvvXZrY38Fxou6Yp8Gd3fzlfsYqIyLY0HoSISCNW1XgQupNaRERiKUGIiEgsJQgREYmlBCEiIrGUIEREJJYShIiIxFKCEBGRWEoQIiISSwlCRERiKUGIiEgsJQgREYmlBCEiIrGUIEREJJYShIiIxFKCEBGRWEoQIiISSwlCRERiKUGIiEgsJQgREYmlBCEiIrGUIEREJJYShIiIxFKCEBGRWEoQIiISSwlCRERiKUGIiEgsJQgREYmlBCEiIrGUIEREJFbTYgcgIg3D+vXrqaysZM2aNcUORWK0aNGCTp060axZs4zXUYIQkZyorKykTZs2lJaWYmbFDkeSuDuLFy+msrKSLl26ZLyeqphEJCfWrFlDu3btlBzqIDOjXbt2NS7dKUGISM4oOdRdtfnbKEGIiEgsJQgRKYrRo6G0FJo0Cc+jR9d+W4sXL6Z79+50796d3XbbjT322GPz+3Xr1lW5bnl5OVdeeWW1++jdu3ftA6yn1EgtIgU3ejQMGQKrVoX38+aF9wCDBtV8e+3atWPatGkA3HrrrbRu3Zqrr7568/wNGzbQtGn86a6srIyysrJq9/Hmm2/WPLB6Lq8lCDPrZ2azzWyOmV0XM39HM3vBzKab2SwzuzDTdUWk/rrhhi3JIWHVqjA9Vy644AJ++ctfcuyxx3LttdcyadIkevfuzWGHHUbv3r2ZPXs2AP/4xz849dRTgZBcLrroIvr27cvee+/Nfffdt3l7rVu33rx83759Oeuss+jatSuDBg3C3QF48cUX6dq1K3369OHKK6/cvN1kFRUVHHXUUfTo0YMePXpslXjuvPNODjnkELp168Z114XT3pw5czj++OPp1q0bPXr04JNPPsndQapG3koQZlYC3A+cAFQCk81svLu/n7TY5cD77v4DM+sAzDaz0cDGDNYVkXpq/vyaTa+tjz76iNdee42SkhKWL1/OxIkTadq0Ka+99hrXX389zzzzzDbrfPjhh/z9739nxYoVHHDAAQwbNmybewfeffddZs2axe67786RRx7Jv//9b8rKyrj00kuZOHEiXbp0YeDAgbEx7bLLLrz66qu0aNGCjz/+mIEDB1JeXs5LL73E888/zzvvvEPLli1ZsmQJAIMGDeK6667jjDPOYM2aNWzatCm3B6kK+axi6gXMcfe5AGY2BjgNSD7JO9DGQvN6a2AJsAE4PIN1RaSe2muvUK0UNz2Xzj77bEpKSgBYtmwZgwcP5uOPP8bMWL9+few6/fv3p3nz5jRv3pxddtmFhQsX0qlTp62W6dWr1+Zp3bt3p6KigtatW7P33ntvvs9g4MCBjBw5cpvtr1+/niuuuIJp06ZRUlLCRx99BMBrr73GhRdeSMuWLQHYeeedWbFiBZ9//jlnnHEGEG52K6R8VjHtAXyW9L4ympZsBHAgsACYCfzc3TdluK6I1FPDh0N0HtysZcswPZdatWq1+fVNN93Esccey3vvvccLL7yQ9p6A5s2bb35dUlLChg0bMlomUc1UnXvuuYddd92V6dOnU15evrkR3d23uRQ1023mSz4TRNxFt6mf9iRgGrA70B0YYWY7ZLhu2InZEDMrN7PyRYsW1T5aESmYQYNg5Ejo3BnMwvPIkbVroM7UsmXL2GOP8DvzT3/6U86337VrV+bOnUtFRQUAY8eOTRtHx44dadKkCaNGjWLjxo0AnHjiiTz22GOsihpnlixZwg477ECnTp14/vnnAVi7du3m+YWQzwRRCeyZ9L4ToaSQ7ELgWQ/mAJ8CXTNcFwB3H+nuZe5e1qFDh5wFLyL5NWgQVFTApk3hOZ/JAeCaa67h17/+NUceeeTmk3Iubb/99jzwwAP069ePPn36sOuuu7Ljjjtus9xll13GE088wRFHHMFHH320uZTTr18/BgwYQFlZGd27d+fuu+8GYNSoUdx3330ceuih9O7dmy+//DLnsadj+SrCmFlT4CPg+8DnwGTgPHeflbTMg8BCd7/VzHYFpgLdgKXVrRunrKzMy8vL8/BpRKQ6H3zwAQceeGCxwyiqlStX0rp1a9ydyy+/nP32249f/OIXxQ5rs7i/kZlNcffY63zzVoJw9w3AFcDfgA+Ace4+y8yGmtnQaLHbgN5mNhOYAFzr7l+nWzdfsYqI5MLDDz9M9+7dOfjgg1m2bBmXXnppsUPKSt5KEMWgEoRI8agEUffVmRKEiIjUb0oQIiISSwlCRERiKUGIiEgsJQgRqff69u3L3/72t62m3XvvvVx22WVVrpO4qOWUU05h6dKl2yxz6623br4fIZ3nn3+e99/f0gvQzTffzGuvvVaD6OsuJQgRqfcGDhzImDFjtpo2ZsyYtB3mpXrxxRdp27ZtrfadmiB++9vfcvzxx9dqW3WNxoMQkZy76iqIhmfIme7d4d574+edddZZ3Hjjjaxdu5bmzZtTUVHBggUL6NOnD8OGDWPy5MmsXr2as846i9/85jfbrF9aWkp5eTnt27dn+PDhPPnkk+y555506NCBnj17AuEeh5EjR7Ju3Tr23XdfRo0axbRp0xg/fjxvvPEGt99+O8888wy33XYbp556KmeddRYTJkzg6quvZsOGDXz3u9/lwQcfpHnz5pSWljJ48GBeeOEF1q9fz9NPP03Xrl23iqmiooLzzz+fb7/9FoARI0ZsHrTozjvvZNSoUTRp0oSTTz6ZO+64gzlz5jB06FAWLVpESUkJTz/9NPvss09Wx1wlCBGp99q1a0evXr14+eWXgVB6OOecczAzhg8fTnl5OTNmzOCNN95gxowZabczZcoUxowZw7vvvsuzzz7L5MmTN88788wzmTx5MtOnT+fAAw/k0UcfpXfv3gwYMIC77rqLadOmbXVCXrNmDRdccAFjx45l5syZbNiwgQcffHDz/Pbt2zN16lSGDRsWW42V6BZ86tSpjB07dvOod8ndgk+fPp1rrrkGCN2CX3755UyfPp0333yTjh07ZndQUQlCRPIg3S/9fEpUM5122mmMGTOGxx57DIBx48YxcuRINmzYwBdffMH777/PoYceGruNf/7zn5xxxhmbu9weMGDA5nnvvfceN954I0uXLmXlypWcdNJJVcYze/ZsunTpwv777w/A4MGDuf/++7nqqquAkHAAevbsybPPPrvN+nWhW/BGX4LI5bi4IlI8p59+OhMmTGDq1KmsXr2aHj168Omnn3L33XczYcIEZsyYQf/+/dN2852Q2uV2wgUXXMCIESOYOXMmt9xyS7Xbqa6XikSX4em6FK8L3YI36gSRGBd33jxw3zIurpKESP3TunVr+vbty0UXXbS5cXr58uW0atWKHXfckYULF/LSSy9VuY2jjz6a5557jtWrV7NixQpeeOGFzfNWrFhBx44dWb9+PaOTThJt2rRhxYoV22yra9euVFRUMGfOHCD0ynrMMcdk/HnqQrfgjTpBFGJcXBEpnIEDBzJ9+nTOPfdcALp168Zhhx3GwQcfzEUXXcSRRx5Z5fo9evTgnHPOoXv37vzwhz/kqKOO2jzvtttu4/DDD+eEE07YqkH53HPP5a677uKwww7barzoFi1a8Pjjj3P22WdzyCGH0KRJE4YOHUqm6kK34I26s74mTULJIZVZ6KNeRDKnzvrqPnXWVwPpxr/N9bi4IiL1UaNOEIUaF1dEpD5q1AmiGOPiijRkDanKuqGpzd+m0d8HMWiQEoJILrRo0YLFixfTrl27tJeKSnG4O4sXL67x/RGNPkGISG506tSJyspKFi1aVOxQJEaLFi3o1KlTjdZRghCRnGjWrBldunQpdhiSQ426DUJERNJTghARkVhKECIiEqtB3UltZouAecWOI432wNfFDqIKii87ii87ii872cTX2d07xM1oUAmiLjOz8nS3s9cFii87ii87ii87+YpPVUwiIhJLCUJERGIpQRTOyGIHUA3Flx3Flx3Fl528xKc2CBERiaUShIiIxFKCEBGRWEoQOWRme5rZ383sAzObZWY/j1mmr5ktM7Np0ePmAsdYYWYzo31vM/yeBfeZ2Rwzm2FmPQoY2wFJx2WamS03s6tSlino8TOzx8zsKzN7L2nazmb2qpl9HD3vlGbdfmY2OzqW1xUwvrvM7MPo7/ecmbVNs26V34U8xnermX2e9Dc8Jc26xTp+Y5NiqzCzaWnWLcTxiz2nFOw76O565OgBdAR6RK/bAB8BB6Us0xf4SxFjrADaVzH/FOAlwIAjgHeKFGcJ8CXhJp6iHT/gaKAH8F7StDuB66LX1wG/SxP/J8DewHbA9NTvQh7jOxFoGr3+XVx8mXwX8hjfrcDVGfz9i3L8Uub/Hri5iMcv9pxSqO+gShA55O5fuPvU6PUK4ANgj+JGVWOnAU968DbQ1sw6FiGO7wOfuHtR74x394nAkpTJpwFPRK+fAE6PWbUXMMfd57r7OmBMtF7e43P3V9x9Q/T2baBmfTznUJrjl4miHb8EC4Na/Aj4n1zvN1NVnFMK8h1UgsgTMysFDgPeiZn9PTObbmYvmdnBhY0MB14xsylmNiRm/h7AZ0nvKylOkjuX9P+YxTx+ALu6+xcQ/oGBXWKWqSvH8SJCiTBOdd+FfLoiqgJ7LE31SF04fkcBC9394zTzC3r8Us4pBfkOKkHkgZm1Bp4BrnL35SmzpxKqTboB/w08X+DwjnT3HsDJwOVmdnTK/LihwAp6LbSZbQcMAJ6OmV3s45epunAcbwA2AKPTLFLddyFfHgT2AboDXxCqcVIV/fgBA6m69FCw41fNOSXtajHTanQMlSByzMyaEf6Qo9392dT57r7c3VdGr18EmplZ+0LF5+4LouevgOcIxdBklcCeSe87AQsKE91mJwNT3X1h6oxiH7/IwkS1W/T8VcwyRT2OZjYYOBUY5FGFdKoMvgt54e4L3X2ju28CHk6z32Ifv6bAmcDYdMsU6vilOacU5DuoBJFDUZ3lo8AH7v6HNMvsFi2HmfUi/A0WFyi+VmbWJvGa0Jj5Xspi44GfWHAEsCxRlC2gtL/cinn8kowHBkevBwP/F7PMZGA/M+sSlYjOjdbLOzPrB1wLDHD3VWmWyeS7kK/4ktu0zkiz36Idv8jxwIfuXhk3s1DHr4pzSmG+g/lsgW9sD6APoQg3A5gWPU4BhgJDo2WuAGYRrih4G+hdwPj2jvY7PYrhhmh6cnwG3E+4+mEmUFbgY9iScMLfMWla0Y4fIVF9Aawn/CK7GGgHTAA+jp53jpbdHXgxad1TCFedfJI41gWKbw6h7jnxHXwoNb5034UCxTcq+m7NIJywOtal4xdN/1PiO5e0bDGOX7pzSkG+g+pqQ0REYqmKSUREYilBiIhILCUIERGJpQQhIiKxlCBERCSWEoRINcxso23dy2zOehY1s9LknkRF6pKmxQ5ApB5Y7e7dix2ESKGpBCFSS9F4AL8zs0nRY99oemczmxB1RjfBzPaKpu9qYXyG6dGjd7SpEjN7OOrv/xUz2z5a/kozez/azpgifUxpxJQgRKq3fUoV0zlJ85a7ey9gBHBvNG0Eocv0Qwkd5d0XTb8PeMNDR4M9CHfgAuwH3O/uBwNLgR9G068DDou2MzQ/H00kPd1JLVINM1vp7q1jplcAx7n73KhDtS/dvZ2ZfU3oPmJ9NP0Ld29vZouATu6+NmkbpcCr7r5f9P5aoJm7325mLwMrCT3WPu9RJ4UihaIShEh2PM3rdMvEWZv0eiNb2gb7E/rF6glMiXoYFSkYJQiR7JyT9PxW9PpNQs+ZAIOAf0WvJwDDAMysxMx2SLdRM2sC7OnufweuAdoC25RiRPJJv0hEqre9bT1w/cvunrjUtbmZvUP4sTUwmnYl8JiZ/QpYBFwYTf85MNLMLiaUFIYRehKNUwI8ZWY7EnrYvcfdl+bo84hkRG0QIrUUtUGUufvXxY5FJB9UxSQiIrFUghARkVgqQYiISCwlCBERiaUEISIisZQgREQklhKEiIjE+v9JPO1T3mCsNwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the Accuracy\n",
    "\n",
    "plt.clf()   # clear figure\n",
    "acc_values = history_dict['binary_accuracy'] #TO DO: take from the history dictionary of the model the training accuracy\n",
    "val_acc_values = history_dict['val_binary_accuracy'] # TO DO: take from the history dictionary of the model the validation accuracy\n",
    "\n",
    "# Plot Epochs vs Training accuracy and Epochs vs Validation accuracy\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc') #TO DO: use training accuracy\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')#TO DO: use validation accuracy\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What can you conclude from these results? Try to analyse and understand them.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the training loss decreases with every epoch and the training accuracy increases with every epoch. That's what you would expect when running gradient descent optimization -- the quantity you are trying to minimize should get lower with every iteration. But that isn't the case for the validation loss and accuracy: they seem to peak at the fourth epoch. This is an example of what we were warning against earlier: a model that performs better on the training data isn't necessarily a model that will do better on data it has never seen before. In precise terms, what you are seeing is \"overfitting\": after the second epoch, we are over-optimizing on the training data, and we ended up learning representations that are specific to the training data and do not generalize to data outside of the training set.\n",
    "In this case, to prevent overfitting, we could simply stop training after three epochs.\n",
    "\n",
    "Let's train a new network from scratch for four epochs, then evaluate it on our test data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train a new network from scratch for 4 epochs and evaluate it on the test data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x14ba17560> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x14ba17560> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "49/49 [==============================] - 3s 36ms/step - loss: 0.4864 - accuracy: 0.7988\n",
      "Epoch 2/4\n",
      "49/49 [==============================] - 1s 18ms/step - loss: 0.2728 - accuracy: 0.9068\n",
      "Epoch 3/4\n",
      "49/49 [==============================] - 1s 18ms/step - loss: 0.2067 - accuracy: 0.9278\n",
      "Epoch 4/4\n",
      "49/49 [==============================] - 1s 19ms/step - loss: 0.1713 - accuracy: 0.9400\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x14f6b59e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x14f6b59e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.2980 - accuracy: 0.8819\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=4, batch_size=512)\n",
    "results = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.2979873716831207, 0.8819199800491333]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Make predictions with your model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x14e675170> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x14e675170> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.17589027],\n",
       "       [0.99996674],\n",
       "       [0.8519112 ],\n",
       "       ...,\n",
       "       [0.15770543],\n",
       "       [0.07929331],\n",
       "       [0.5958747 ]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further experiments\n",
    "\n",
    "**NOTE: In the next Lab, we will focus more in optimizing the hyperparameters and understanding how they may influence the outcomes. However here are some simple experiments you can also try now.**\n",
    "\n",
    "**Try out these experiments and every time re-compute the accuracy to check how it changes.**\n",
    "#### Experiment 1\n",
    "We were using 2 hidden layers, try to use 1 or 3 hidden layers and see how it affects validation and test accuracy.\n",
    "#### Experiment 2\n",
    "Try to use layers with more hidden units or less hidden units: 32 units, 64 units...\n",
    "#### Experiment 3\n",
    "Try to use the another loss function.\n",
    "#### Experiment 4\n",
    "Try to use the tanh activation instead of ReLU.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Conclusions\n",
    "Here's what you should take away from this example:\n",
    "\n",
    "- There's usually quite a bit of preprocessing you need to do on your raw data in order to be able to feed it -- as tensors -- into a neural network. In the case of sequences of words, they can be encoded as binary vectors -- but there are other encoding options too.\n",
    "- Stacks of Dense layers with relu activations can solve a wide range of problems (including sentiment classification), and you will likely use them frequently.\n",
    "- In a binary classification problem (two output classes), your network should end with a Dense layer with 1 unit and a sigmoid activation, i.e. the output of your network should be a scalar between 0 and 1, encoding a probability.\n",
    "- With such a scalar sigmoid output, on a binary classification problem, the loss function you should use is binary_crossentropy.\n",
    "- The RMSprop optimizer is generally a good enough choice of optimizer, whatever your problem.\n",
    "- As they get better on their training data, neural networks eventually start overfitting and end up obtaining increasingly worse results on data never-seen-before. Make sure to always monitor performance on data that is outside of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
